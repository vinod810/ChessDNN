import numpy as np
import zstandard as zstd
import io
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Any
import random
import os
import platform
import gc
import time
import glob
import multiprocessing as mp
from multiprocessing import Process, Queue, Event, Value
from collections import deque
import threading
import ctypes
import chess
from typing import List, Tuple, Dict, Optional
import struct

from nn_inference import NNUENetwork, DNNNetwork, \
    NNUE_INPUT_SIZE, DNN_INPUT_SIZE, NNUE_HIDDEN_SIZE, \
    DNN_HIDDEN_LAYERS, NNUEFeatures, MAX_SCORE, \
    TANH_SCALE, DNNFeatures

MATE_FACTOR = 100
MAX_MATE_DEPTH = 10
MAX_NON_MATE_SCORE = MAX_SCORE - MAX_MATE_DEPTH * MATE_FACTOR

"""
Chess Neural Network Training Script
Supports both NNUE and DNN architectures for chess position evaluation.
Uses Stockfish binpack training data format.

BINPACK Format:
    - Uses .binpack or .bin files (Stockfish training data format)
    - Highly efficient binary format used by Stockfish NNUE training
    - Fast data loading
    - Compatible with data generated by Stockfish's training tools

ARCHITECTURE SELECTION:
    Set nn_type = "NNUE" or nn_type = "DNN" in the Configuration section below.

NNUE (Efficiently Updatable Neural Network):
    - Input: Two 40960-dimensional sparse vectors (white/black king-piece features)
    - Architecture: 40960 -> 256 (shared) -> 512 (concatenated) -> 32 -> 32 -> 1
    - Hidden activation: Clipped ReLU [0, 1]
    - Output activation: Linear (no activation)
    - Features: King-relative piece positions for both perspectives
    - Output: Position evaluation from side-to-move's perspective

DNN (Deep Neural Network):
    - Input: Single 768-dimensional one-hot encoded vector (from player's perspective)
    - Architecture: 768 -> 1024 -> 256 -> 32 -> 1
    - Hidden activation: Clipped ReLU [0, 1]
    - Output activation: Linear (no activation)
    - Features: Piece positions from perspective of player to move
    - Output: Position evaluation from side-to-move's perspective

Both networks are trained on tanh-normalized targets: tanh(centipawns / 400).
Both networks output linearly and learn to produce values in approximately [-1, 1].
Output range: approximately [-1, 1] for both architectures.
"""

# Configuration
# Network type selection: "NNUE" or "DNN"
NN_TYPE = "NNUE"

INPUT_SIZE = NNUE_INPUT_SIZE if NN_TYPE == "NNUE" else DNN_INPUT_SIZE
FIRST_HIDDEN_SIZE = NNUE_HIDDEN_SIZE if NN_TYPE == "NNUE" else DNN_HIDDEN_LAYERS[0]
MODEL_PATH = "model/nnue.pt" if NN_TYPE == "NNUE" else "model/dnn.pt"

# Main configuration
BATCH_SIZE = 32 #8192 * 2
if platform.system() == "Windows":
    QUEUE_READ_TIMEOUT = int(BATCH_SIZE / 512) * 5  # Windows file reading is slow
else:
    QUEUE_READ_TIMEOUT = 10 # int(BATCH_SIZE / 512)

# Worker configuration
QUEUE_MAX_SIZE = 100  # Max batches in queue
SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 10

# Misc
GC_INTERVAL = 1000  # Run garbage collection every N batches

# Training
LEARNING_RATE = 0.001
VALIDATION_SPLIT = 0.05
STEPS_PER_EPOCH = 1000
POSITIONS_PER_EPOCH = BATCH_SIZE * STEPS_PER_EPOCH
EPOCHS = 500
EARLY_STOPPING_PATIENCE = 10
LR_PATIENCE = 3

# Binpack configuration
BINPACK_SKIP_RATE = 0  # Skip this fraction of positions (0 = use all, 0.5 = use half)
BINPACK_FILTER_CHECKS = True  # Skip positions where side to move is in check
BINPACK_MIN_PLY = 16  # Skip positions before this ply (opening filter)

# Sparse input optimization (faster for NNUE due to high sparsity)
USE_SPARSE_INPUT = True  # Use sparse tensors for first layer - much faster for NNUE


# ==============================================================================
# STOCKFISH BINPACK FORMAT SUPPORT
# ==============================================================================

class BinpackReader:
    """
    Reader for Stockfish NNUE training data in binpack format.

    Binpack format stores chess training positions in a highly compressed binary format:
    - Each entry contains: position, score, move, ply, result
    - Positions are stored using Stockfish's packed board representation
    - Much more efficient than PGN for training data

    The format uses a packed board representation where each position is stored as:
    - 32 bytes for piece positions (4 bits per square for 64 squares)
    - Additional bytes for castling rights, en passant, side to move
    - Score as int16
    - Move as uint16
    - Ply as uint16
    - Result as int8
    """

    # Piece encoding in binpack format
    PIECE_NONE = 0
    PIECE_PAWN = 1
    PIECE_KNIGHT = 2
    PIECE_BISHOP = 3
    PIECE_ROOK = 4
    PIECE_QUEEN = 5
    PIECE_KING = 6

    # Entry size in bytes for the standard binpack format
    # Position: 32 bytes (packed), metadata: ~8 bytes
    SFEN_PACKED_SIZE = 32

    # Standard training entry structure
    # This matches Stockfish's PackedSfenValue structure
    ENTRY_SIZE = 40  # packed sfen (32) + score (2) + move (2) + ply (2) + result (1) + padding (1)

    def __init__(self, filepath: str, skip_rate: float = 0.0):
        """
        Initialize binpack reader.

        Args:
            filepath: Path to .binpack or .bin file
            skip_rate: Fraction of positions to randomly skip (0-1)
        """
        self.filepath = filepath
        self.skip_rate = skip_rate
        self.file = None
        self.file_size = 0
        self.num_entries = 0

    def open(self):
        """Open the binpack file and determine size."""
        self.file = open(self.filepath, 'rb')
        self.file.seek(0, 2)  # Seek to end
        self.file_size = self.file.tell()
        self.file.seek(0)  # Back to start

        # Calculate number of entries
        self.num_entries = self.file_size // self.ENTRY_SIZE

    def close(self):
        """Close the file."""
        if self.file:
            self.file.close()
            self.file = None

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def seek_to_entry(self, index: int):
        """Seek to a specific entry by index."""
        self.file.seek(index * self.ENTRY_SIZE)

    def read_entry(self) -> Optional[Tuple[chess.Board, int, int, int, int]]:
        """
        Read a single entry from the binpack file.

        Returns:
            Tuple of (board, score_cp, move, ply, result) or None if EOF
            - board: chess.Board object
            - score_cp: centipawn score from white's perspective
            - move: move as integer (can be decoded)
            - ply: ply number in game
            - result: game result (-1, 0, 1)
        """
        data = self.file.read(self.ENTRY_SIZE)
        if len(data) < self.ENTRY_SIZE:
            return None

        return self._parse_entry(data)

    def _parse_entry(self, data: bytes) -> Optional[Tuple[chess.Board, int, int, int, int]]:
        """Parse a single binpack entry."""
        try:
            # Extract packed sfen (32 bytes) and metadata
            packed_sfen = data[:32]
            score = struct.unpack('<h', data[32:34])[0]  # int16, little-endian
            move = struct.unpack('<H', data[34:36])[0]  # uint16, little-endian
            ply = struct.unpack('<H', data[36:38])[0]  # uint16, little-endian
            result = struct.unpack('<b', data[38:39])[0]  # int8

            # Parse the packed sfen to get a board
            board = self._unpack_sfen(packed_sfen)
            if board is None:
                return None

            return (board, score, move, ply, result)

        except Exception as e:
            return None

    def _unpack_sfen(self, packed: bytes) -> Optional[chess.Board]:
        """
        Unpack Stockfish's packed sfen format to a chess.Board.

        The packed format is:
        - Bytes 0-31: Piece positions (4 bits per square, 2 squares per byte)
          Square ordering: a1, b1, c1, ..., h8 (file-major, rank-minor)
          Piece encoding: 0=none, 1-6=white pawn-king, 9-14=black pawn-king
        """
        try:
            board = chess.Board(fen=None)
            board.clear()

            # Decode pieces from packed bytes
            # Each byte contains 2 squares (4 bits each)
            for byte_idx in range(32):
                byte_val = packed[byte_idx]

                # First square (lower 4 bits)
                sq1 = byte_idx * 2
                piece1 = byte_val & 0x0F
                if piece1 != 0:
                    color1, ptype1 = self._decode_piece(piece1)
                    if ptype1 is not None:
                        board.set_piece_at(sq1, chess.Piece(ptype1, color1))

                # Second square (upper 4 bits)
                sq2 = byte_idx * 2 + 1
                piece2 = (byte_val >> 4) & 0x0F
                if piece2 != 0:
                    color2, ptype2 = self._decode_piece(piece2)
                    if ptype2 is not None:
                        board.set_piece_at(sq2, chess.Piece(ptype2, color2))

            # Simple heuristic: check if position looks valid
            white_king = board.king(chess.WHITE)
            black_king = board.king(chess.BLACK)

            if white_king is None or black_king is None:
                return None

            # Default to white to move (will be corrected by caller if needed)
            board.turn = chess.WHITE

            # Reset castling rights based on king/rook positions
            board.castling_rights = chess.BB_EMPTY
            if white_king == chess.E1:
                if board.piece_at(chess.H1) == chess.Piece(chess.ROOK, chess.WHITE):
                    board.castling_rights |= chess.BB_H1
                if board.piece_at(chess.A1) == chess.Piece(chess.ROOK, chess.WHITE):
                    board.castling_rights |= chess.BB_A1
            if black_king == chess.E8:
                if board.piece_at(chess.H8) == chess.Piece(chess.ROOK, chess.BLACK):
                    board.castling_rights |= chess.BB_H8
                if board.piece_at(chess.A8) == chess.Piece(chess.ROOK, chess.BLACK):
                    board.castling_rights |= chess.BB_A8

            return board

        except Exception as e:
            return None

    def _decode_piece(self, encoded: int) -> Tuple[bool, Optional[int]]:
        """
        Decode piece from 4-bit encoding.

        Standard Stockfish encoding:
        0 = no piece
        1-6 = white pawn, knight, bishop, rook, queen, king
        9-14 = black pawn, knight, bishop, rook, queen, king

        Returns: (color, piece_type) where color is True for white
        """
        if encoded == 0:
            return (True, None)

        if 1 <= encoded <= 6:
            # White piece
            piece_types = [None, chess.PAWN, chess.KNIGHT, chess.BISHOP,
                           chess.ROOK, chess.QUEEN, chess.KING]
            return (chess.WHITE, piece_types[encoded])
        elif 9 <= encoded <= 14:
            # Black piece
            piece_types = [None, chess.PAWN, chess.KNIGHT, chess.BISHOP,
                           chess.ROOK, chess.QUEEN, chess.KING]
            return (chess.BLACK, piece_types[encoded - 8])
        else:
            return (True, None)

    def iter_entries(self, max_entries: Optional[int] = None):
        """
        Iterate over entries in the binpack file.

        Args:
            max_entries: Maximum number of entries to read (None = all)

        Yields:
            Tuples of (board, score_cp, move, ply, result)
        """
        count = 0
        while True:
            if max_entries is not None and count >= max_entries:
                break

            entry = self.read_entry()
            if entry is None:
                break

            # Apply skip rate
            if self.skip_rate > 0 and random.random() < self.skip_rate:
                continue

            count += 1
            yield entry


class PlainTextBinpackReader:
    """
    Alternative reader for text-based Stockfish training data.

    Some Stockfish training data is stored as plain text with format:
    fen <FEN> score <SCORE> ply <PLY> result <r>

    Or simple format:
    <FEN>;<SCORE>
    """

    def __init__(self, filepath: str, skip_rate: float = 0.0):
        self.filepath = filepath
        self.skip_rate = skip_rate
        self.file = None
        #self.is_compressed = filepath.endswith('.zst')

    def open(self):
        if self.is_compressed:
            self.raw_file = open(self.filepath, 'rb')
            dctx = zstd.ZstdDecompressor()
            self.reader = dctx.stream_reader(self.raw_file)
            self.file = io.TextIOWrapper(self.reader, encoding='utf-8')
        else:
            self.file = open(self.filepath, 'r', encoding='utf-8')
            self.raw_file = None
            self.reader = None

    def close(self):
        if self.file:
            self.file.close()
        if hasattr(self, 'reader') and self.reader:
            self.reader.close()
        if hasattr(self, 'raw_file') and self.raw_file:
            self.raw_file.close()

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def iter_entries(self, max_entries: Optional[int] = None):
        """Iterate over entries in the text file."""
        count = 0

        for line in self.file:
            if max_entries is not None and count >= max_entries:
                break

            line = line.strip()
            if not line:
                continue

            # Apply skip rate
            if self.skip_rate > 0 and random.random() < self.skip_rate:
                continue

            entry = self._parse_line(line)
            if entry is not None:
                count += 1
                yield entry

    def _parse_line(self, line: str) -> Optional[Tuple[chess.Board, int, int, int, int]]:
        """Parse a single line of text training data."""
        try:
            # Try format: "fen <FEN> score <SCORE> ply <PLY> result <r>"
            if line.startswith('fen '):
                parts = line.split()
                # Find indices of keywords
                fen_parts = []
                score = 0
                ply = 0
                result = 0

                i = 1  # Skip 'fen'
                while i < len(parts):
                    if parts[i] == 'score':
                        score = int(parts[i + 1])
                        i += 2
                    elif parts[i] == 'ply':
                        ply = int(parts[i + 1])
                        i += 2
                    elif parts[i] == 'result':
                        result = int(parts[i + 1])
                        i += 2
                    elif parts[i] == 'move':
                        i += 2  # Skip move
                    else:
                        fen_parts.append(parts[i])
                        i += 1

                fen = ' '.join(fen_parts)
                board = chess.Board(fen)
                return (board, score, 0, ply, result)

            # Try simple format: "<FEN>;<SCORE>"
            elif ';' in line:
                fen, score_str = line.rsplit(';', 1)
                board = chess.Board(fen.strip())
                score = int(score_str.strip())
                return (board, score, 0, 0, 0)

            # Try tab-separated: "<FEN>\t<SCORE>"
            elif '\t' in line:
                parts = line.split('\t')
                if len(parts) >= 2:
                    board = chess.Board(parts[0].strip())
                    score = int(parts[1].strip())
                    return (board, score, 0, 0, 0)

            return None

        except Exception as e:
            return None


def get_binpack_reader(filepath: str, skip_rate: float = 0.0):
    """
    Factory function to get the appropriate binpack reader based on file extension.
    """
    ext = os.path.splitext(filepath)[1].lower()

    if ext in ['.binpack', '.bin']:
        return BinpackReader(filepath, skip_rate)
    #elif ext in ['.txt', '.plain', '.zst']:
    #    return PlainTextBinpackReader(filepath, skip_rate)
    else:
        # Try binary first, fall back to text
        return BinpackReader(filepath, skip_rate)


# ==============================================================================
# SHARED STATS AND ENCODING
# ==============================================================================

class SharedStats:
    """Thread-safe shared statistics using multiprocessing Values"""

    def __init__(self, num_workers: int):
        self.num_workers = num_workers

        # Worker stats (arrays indexed by worker_id)
        self.worker_entries = [Value(ctypes.c_uint64, 0) for _ in range(num_workers)]
        self.worker_positions = [Value(ctypes.c_uint64, 0) for _ in range(num_workers)]
        self.worker_batches = [Value(ctypes.c_uint64, 0) for _ in range(num_workers)]
        self.worker_file_loops = [Value(ctypes.c_uint64, 0) for _ in range(num_workers)]
        self.worker_wait_ms = [Value(ctypes.c_uint64, 0) for _ in range(num_workers)]
        self.worker_process_ms = [Value(ctypes.c_uint64, 0) for _ in range(num_workers)]

        # Main process stats
        self.main_batches = Value(ctypes.c_uint64, 0)
        self.main_train_batches = Value(ctypes.c_uint64, 0)
        self.main_val_batches = Value(ctypes.c_uint64, 0)
        self.main_wait_ms = Value(ctypes.c_uint64, 0)
        self.main_process_ms = Value(ctypes.c_uint64, 0)

        # Queue stats
        self.queue_full_count = Value(ctypes.c_uint64, 0)
        self.queue_empty_count = Value(ctypes.c_uint64, 0)

    def get_worker_stats(self, worker_id: int) -> Dict[str, Any]:
        return {
            'entries': self.worker_entries[worker_id].value,
            'positions': self.worker_positions[worker_id].value,
            'batches': self.worker_batches[worker_id].value,
            'file_loops': self.worker_file_loops[worker_id].value,
            'wait_seconds': self.worker_wait_ms[worker_id].value / 1000.0,
            'process_seconds': self.worker_process_ms[worker_id].value / 1000.0,
        }

    def get_main_stats(self) -> Dict[str, Any]:
        return {
            'batches': self.main_batches.value,
            'train_batches': self.main_train_batches.value,
            'val_batches': self.main_val_batches.value,
            'wait_seconds': self.main_wait_ms.value / 1000.0,
            'process_seconds': self.main_process_ms.value / 1000.0,
        }

    def print_stats(self, file_paths: List[str]):
        """Print formatted statistics"""
        print("\n" + "=" * 80)
        print("PERFORMANCE STATISTICS")
        print("=" * 80)

        total_worker_wait = 0
        total_worker_process = 0

        for i in range(self.num_workers):
            stats = self.get_worker_stats(i)
            total_worker_wait += stats['wait_seconds']
            total_worker_process += stats['process_seconds']

            file_name = os.path.basename(file_paths[i]) if i < len(file_paths) else f"worker_{i}"
            print(f"\nWorker {i} ({file_name}):")
            print(f"  Entries: {stats['entries']:,} | Positions: {stats['positions']:,} | "
                  f"Batches: {stats['batches']:,}")
            print(f"  File loops: {stats['file_loops']} | "
                  f"Wait: {stats['wait_seconds']:.1f}s | Process: {stats['process_seconds']:.1f}s")

            if stats['wait_seconds'] + stats['process_seconds'] > 0:
                wait_pct = stats['wait_seconds'] / (stats['wait_seconds'] + stats['process_seconds']) * 100
                print(f"  Wait ratio: {wait_pct:.1f}%")

        main_stats = self.get_main_stats()
        print(f"\nMain Process:")
        print(f"  Batches consumed: {main_stats['batches']:,} "
              f"(Train: {main_stats['train_batches']:,}, Val: {main_stats['val_batches']:,})")
        print(f"  Wait: {main_stats['wait_seconds']:.1f}s | Process: {main_stats['process_seconds']:.1f}s")

        if main_stats['wait_seconds'] + main_stats['process_seconds'] > 0:
            wait_pct = main_stats['wait_seconds'] / (main_stats['wait_seconds'] + main_stats['process_seconds']) * 100
            print(f"  Wait ratio: {wait_pct:.1f}%")

        print(f"\nQueue Events:")
        print(f"  Queue full events: {self.queue_full_count.value:,}")
        print(f"  Queue empty events: {self.queue_empty_count.value:,}")

        # Analysis
        print(f"\nANALYSIS:")
        avg_worker_wait = total_worker_wait / max(1, self.num_workers)
        if avg_worker_wait > main_stats['wait_seconds'] * 1.5:
            print("  ⚠ Workers waiting more than main - queue may be full often")
            print("    Consider: increase QUEUE_MAX_SIZE or reduce worker count")
        elif main_stats['wait_seconds'] > avg_worker_wait * 1.5:
            print("  ⚠ Main process waiting more than workers - queue often empty")
            print("    Consider: add more workers or increase batch size")
        else:
            print("  ✓ Balanced: workers and main process have similar wait times")

        print("=" * 80)


def encode_sparse_batch(positions: List[Tuple]) -> Dict[str, Any]:
    """
    Encode a batch of positions into sparse format for efficient IPC.
    Instead of sending dense tensors, we send indices of non-zero elements.

    For NNUE: Returns dict with white_indices, black_indices, stm, scores, batch_size
    For DNN: Returns dict with features, scores, batch_size (no stm needed as features are from perspective)
    """
    if NN_TYPE == "NNUE":
        white_indices = []
        black_indices = []
        stms = []
        scores = []

        for white_feat, black_feat, stm, score in positions:
            white_indices.append(white_feat)
            black_indices.append(black_feat)
            stms.append(stm)
            scores.append(score)

        return {
            'white_indices': white_indices,
            'black_indices': black_indices,
            'stm': np.array(stms, dtype=np.float32),
            'scores': np.array(scores, dtype=np.float32),
            'batch_size': len(positions)
        }
    else:  # DNN
        features = []
        scores = []

        for feat, score in positions:
            features.append(feat)
            scores.append(score)

        return {
            'features': features,
            'scores': np.array(scores, dtype=np.float32),
            'batch_size': len(positions)
        }


def decode_sparse_batch(batch_data: Dict[str, Any], device: str = 'cpu') -> Tuple[torch.Tensor, ...]:
    """
    Decode sparse batch data back to dense tensors.
    Called in main process after receiving from queue.

    For NNUE: Returns (white_input, black_input, stm, scores)
    For DNN: Returns (features, scores) - note different return signature!
    """
    batch_size = batch_data['batch_size']
    scores = torch.tensor(batch_data['scores'], dtype=torch.float32, device=device).unsqueeze(1)

    if NN_TYPE == "NNUE":
        white_indices = batch_data['white_indices']
        black_indices = batch_data['black_indices']

        white_input = torch.zeros(batch_size, INPUT_SIZE, device=device)
        black_input = torch.zeros(batch_size, INPUT_SIZE, device=device)

        for i in range(batch_size):
            if white_indices[i]:
                white_input[i, white_indices[i]] = 1.0
            if black_indices[i]:
                black_input[i, black_indices[i]] = 1.0

        stm = torch.tensor(batch_data['stm'], dtype=torch.float32, device=device).unsqueeze(1)
        return white_input, black_input, stm, scores
    else:  # DNN
        feature_indices = batch_data['features']

        # Create dense tensor
        features = torch.zeros(batch_size, INPUT_SIZE, device=device)

        for i in range(batch_size):
            if feature_indices[i]:
                features[i, feature_indices[i]] = 1.0

        return features, scores


def decode_sparse_batch_sparse(batch_data: Dict[str, Any], device: str = 'cpu') -> Tuple[torch.Tensor, ...]:
    """
    Decode sparse batch data to sparse COO tensors for efficient first-layer computation.
    Much faster than dense for NNUE due to high sparsity (~30 active features out of 40,960).

    For NNUE: Returns (white_sparse, black_sparse, stm, scores)
    For DNN: Returns (features_sparse, scores)
    """
    batch_size = batch_data['batch_size']
    scores = torch.tensor(batch_data['scores'], dtype=torch.float32, device=device).unsqueeze(1)

    if NN_TYPE == "NNUE":
        white_indices = batch_data['white_indices']
        black_indices = batch_data['black_indices']

        # Build COO indices for white features
        white_row_indices = []
        white_col_indices = []
        for i in range(batch_size):
            if white_indices[i]:
                for idx in white_indices[i]:
                    white_row_indices.append(i)
                    white_col_indices.append(idx)

        # Build COO indices for black features
        black_row_indices = []
        black_col_indices = []
        for i in range(batch_size):
            if black_indices[i]:
                for idx in black_indices[i]:
                    black_row_indices.append(i)
                    black_col_indices.append(idx)

        # Create sparse tensors
        white_indices_tensor = torch.tensor([white_row_indices, white_col_indices], dtype=torch.long, device=device)
        white_values = torch.ones(len(white_row_indices), dtype=torch.float32, device=device)
        white_sparse = torch.sparse_coo_tensor(white_indices_tensor, white_values, (batch_size, INPUT_SIZE))

        black_indices_tensor = torch.tensor([black_row_indices, black_col_indices], dtype=torch.long, device=device)
        black_values = torch.ones(len(black_row_indices), dtype=torch.float32, device=device)
        black_sparse = torch.sparse_coo_tensor(black_indices_tensor, black_values, (batch_size, INPUT_SIZE))

        stm = torch.tensor(batch_data['stm'], dtype=torch.float32, device=device).unsqueeze(1)
        return white_sparse, black_sparse, stm, scores

    else:  # DNN
        feature_indices = batch_data['features']

        # Build COO indices
        row_indices = []
        col_indices = []
        for i in range(batch_size):
            if feature_indices[i]:
                for idx in feature_indices[i]:
                    row_indices.append(i)
                    col_indices.append(idx)

        # Create sparse tensor
        indices_tensor = torch.tensor([row_indices, col_indices], dtype=torch.long, device=device)
        values = torch.ones(len(row_indices), dtype=torch.float32, device=device)
        features_sparse = torch.sparse_coo_tensor(indices_tensor, values, (batch_size, INPUT_SIZE))

        return features_sparse, scores


def sparse_first_layer(sparse_input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Compute first layer using sparse-dense matrix multiplication.

    sparse_input: (batch_size, input_size) sparse COO tensor
    weight: (hidden_size, input_size) dense tensor from nn.Linear
    bias: (hidden_size,) dense tensor from nn.Linear

    Returns: (batch_size, hidden_size) dense tensor
    """
    # sparse @ dense.T -> dense
    # Note: torch.sparse.mm expects (sparse, dense) and computes sparse @ dense
    # We need: input @ weight.T = (batch, input) @ (input, hidden) = (batch, hidden)
    # So we compute: sparse_input @ weight.T
    output = torch.sparse.mm(sparse_input, weight.t())
    output = output + bias
    return output


# ==============================================================================
# BINPACK WORKER PROCESS
# ==============================================================================

def binpack_worker_process(
        worker_id: int,
        binpack_file: str,
        output_queue: Queue,
        stop_event: Event,
        stats: SharedStats,
        nn_type: str,
        batch_size: int = BATCH_SIZE,
        min_ply: int = BINPACK_MIN_PLY,
        filter_checks: bool = BINPACK_FILTER_CHECKS,
        skip_rate: float = BINPACK_SKIP_RATE,
        shuffle_buffer_size: int = SHUFFLE_BUFFER_SIZE
):
    """
    Worker process that streams positions from a binpack file.
    Loops through the file repeatedly until stop_event is set.
    Uses a shuffle buffer to randomize position order.
    """
    print(f"Binpack Worker {worker_id} starting: {os.path.basename(binpack_file)}")

    position_buffer = []
    gc_counter = 0
    entries_processed = 0

    while not stop_event.is_set():
        try:
            # Determine reader type based on file
            reader = get_binpack_reader(binpack_file, skip_rate)

            with reader:
                for entry in reader.iter_entries():
                    if stop_event.is_set():
                        break

                    process_start = time.time()

                    board, score_cp, move, ply, result = entry
                    entries_processed += 1

                    # Apply filters
                    if ply < min_ply:
                        continue

                    if board.is_game_over():
                        continue

                    if filter_checks and board.is_check():
                        continue

                    # Clamp score to reasonable range
                    score_cp = max(-MAX_NON_MATE_SCORE, min(MAX_NON_MATE_SCORE, score_cp))

                    # Convert to side-to-move perspective
                    # Binpack scores are typically from white's perspective
                    if board.turn == chess.BLACK:
                        score_cp_stm = -score_cp
                    else:
                        score_cp_stm = score_cp

                    # Convert to tanh scale
                    score_tanh = np.tanh(score_cp_stm / TANH_SCALE)

                    # Extract features based on network type
                    if nn_type == "NNUE":
                        white_feat, black_feat = NNUEFeatures.board_to_features(board)
                        stm = 1.0 if board.turn == chess.WHITE else 0.0
                        position_buffer.append((white_feat, black_feat, stm, score_tanh))
                    else:  # DNN
                        feat = DNNFeatures.board_to_features(board)
                        position_buffer.append((feat, score_tanh))

                    stats.worker_positions[worker_id].value += 1

                    process_time = time.time() - process_start
                    stats.worker_process_ms[worker_id].value += int(process_time * 1000)

                    # When buffer is large enough, shuffle and send batches
                    while len(position_buffer) >= shuffle_buffer_size and not stop_event.is_set():
                        random.shuffle(position_buffer)

                        while len(position_buffer) >= shuffle_buffer_size // 2 + batch_size and not stop_event.is_set():
                            batch_positions = position_buffer[:batch_size]
                            position_buffer = position_buffer[batch_size:]

                            sparse_batch = encode_sparse_batch(batch_positions)

                            wait_start = time.time()
                            while not stop_event.is_set():
                                try:
                                    output_queue.put(sparse_batch, timeout=0.1)
                                    stats.worker_batches[worker_id].value += 1
                                    break
                                except:
                                    stats.queue_full_count.value += 1

                            wait_time = time.time() - wait_start
                            stats.worker_wait_ms[worker_id].value += int(wait_time * 1000)

                            del batch_positions
                            del sparse_batch

                    # Periodic garbage collection
                    gc_counter += 1
                    if gc_counter >= GC_INTERVAL:
                        gc.collect()
                        gc_counter = 0

                # EOF reached
                stats.worker_file_loops[worker_id].value += 1
                stats.worker_entries[worker_id].value = entries_processed

            gc.collect()

        except Exception as e:
            print(f"Binpack Worker {worker_id} error: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(1)

    # Send remaining positions
    if position_buffer and not stop_event.is_set():
        random.shuffle(position_buffer)
        while len(position_buffer) >= batch_size:
            batch_positions = position_buffer[:batch_size]
            position_buffer = position_buffer[batch_size:]
            sparse_batch = encode_sparse_batch(batch_positions)
            try:
                output_queue.put(sparse_batch, timeout=1.0)
            except:
                break
        if position_buffer:
            sparse_batch = encode_sparse_batch(position_buffer)
            try:
                output_queue.put(sparse_batch, timeout=1.0)
            except:
                pass

    position_buffer.clear()
    gc.collect()

    print(f"Binpack Worker {worker_id} stopped")


# ==============================================================================
# EARLY STOPPING
# ==============================================================================

class EarlyStopping:
    """Early stopping to stop training when validation loss doesn't improve"""

    def __init__(self, patience: int = EARLY_STOPPING_PATIENCE, min_delta: float = 0.0, verbose: bool = True,
                 checkpoint_path: str = MODEL_PATH, optimizer: optim.Optimizer = None):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.checkpoint_path = checkpoint_path
        self.optimizer = optimizer
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_epoch = 0

    def __call__(self, val_loss: float, model: nn.Module, epoch: int, lr: float = None) -> bool:
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_epoch = epoch
            self._save_checkpoint(model, epoch, val_loss, lr)
            if self.verbose:
                print(f"  First validation loss: {val_loss:.6f}")
                print(f"  Saved checkpoint to {self.checkpoint_path}")
            return False

        if val_loss < self.best_loss - self.min_delta:
            improvement = self.best_loss - val_loss
            if self.verbose:
                print(f"  Validation loss improved by {improvement:.6f}")
            self.best_loss = val_loss
            self.best_epoch = epoch
            self._save_checkpoint(model, epoch, val_loss, lr)
            if self.verbose:
                print(f"  Saved checkpoint to {self.checkpoint_path}")
            self.counter = 0
            return False

        self.counter += 1
        if self.verbose:
            print(f"  No improvement for {self.counter}/{self.patience} epochs")

        if self.counter >= self.patience:
            self.early_stop = True
            if self.verbose:
                print(f"  Early stopping triggered! Best loss: {self.best_loss:.6f} (epoch {self.best_epoch})")
            return True

        return False

    def _save_checkpoint(self, model: nn.Module, epoch: int, val_loss: float, lr: float = None):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'val_loss': val_loss,
        }
        if lr is not None:
            checkpoint['lr'] = lr
        if self.optimizer is not None:
            checkpoint['optimizer_state_dict'] = self.optimizer.state_dict()
        torch.save(checkpoint, self.checkpoint_path)

    def restore_best_model(self, model: nn.Module):
        if self.checkpoint_path and self.best_loss is not None:
            checkpoint = torch.load(self.checkpoint_path, weights_only=True)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"Restored model from epoch {checkpoint['epoch']} "
                  f"with validation loss: {checkpoint['val_loss']:.6f}")


# ==============================================================================
# PARALLEL TRAINER
# ==============================================================================

class ParallelTrainer:
    """Main training coordinator with parallel data loading"""

    def __init__(
            self,
            data_dir: str,
            model: nn.Module,
            batch_size: int = BATCH_SIZE,
            validation_split: float = VALIDATION_SPLIT,
            queue_size: int = QUEUE_MAX_SIZE,
            device: str = 'cpu',
            seed: int = 42,
            resume_from: Optional[str] = None,
            use_sparse: bool = USE_SPARSE_INPUT
    ):
        self.data_dir = data_dir
        self.model = model.to(device)
        self.batch_size = batch_size
        self.validation_split = validation_split
        self.queue_size = queue_size
        self.device = device
        self.seed = seed
        self.use_sparse = use_sparse and (NN_TYPE == "NNUE" or NN_TYPE == "DNN")
        self.start_epoch = 0
        self.best_val_loss = None

        # Find binpack files
        patterns = ["*.binpack", "*.bin", "*.plain", "*.txt"]
        self.data_files = []
        for pattern in patterns:
            self.data_files.extend(glob.glob(os.path.join(data_dir, pattern)))
        # Also check for compressed versions
        #self.data_files.extend(glob.glob(os.path.join(data_dir, "*.zst")))
        # Remove duplicates and sort
        self.data_files = sorted(set(self.data_files))

        if not self.data_files:
            raise ValueError(f"No binpack files found in {data_dir}")

        print(f"Found {len(self.data_files)} binpack files:")
        for f in self.data_files:
            print(f"  - {os.path.basename(f)}")

        self.num_workers = len(self.data_files)

        # Multiprocessing components
        self.data_queue = None
        self.stop_event = None
        self.workers = []
        self.stats = None

        # Training state
        self.rng = random.Random(seed)
        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)
        self.criterion = nn.MSELoss()

        # Validation buffer with size limit
        self.val_buffer = deque(maxlen=1000)
        self.val_buffer_lock = threading.Lock()

        # Resume from checkpoint if specified
        if resume_from and os.path.exists(resume_from):
            self._load_checkpoint(resume_from)

    def _load_checkpoint(self, checkpoint_path: str):
        """Load model and optimizer state from checkpoint."""
        print(f"Resuming from checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)

        # Load model state
        self.model.load_state_dict(checkpoint['model_state_dict'])

        # Load optimizer state if available
        if 'optimizer_state_dict' in checkpoint:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print("  Restored optimizer state")

        # Load training progress
        if 'epoch' in checkpoint:
            self.start_epoch = checkpoint['epoch']
            print(f"  Resuming from epoch {self.start_epoch}")

        if 'val_loss' in checkpoint:
            self.best_val_loss = checkpoint['val_loss']
            print(f"  Best validation loss so far: {self.best_val_loss:.6f}")

        if 'lr' in checkpoint:
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = checkpoint['lr']
            print(f"  Restored learning rate: {checkpoint['lr']:.6f}")

    def start_workers(self):
        """Start all worker processes"""
        self.data_queue = mp.Queue(maxsize=self.queue_size)
        self.stop_event = mp.Event()
        self.stats = SharedStats(self.num_workers)

        for i, data_file in enumerate(self.data_files):
            p = Process(
                target=binpack_worker_process,
                args=(i, data_file, self.data_queue, self.stop_event, self.stats, NN_TYPE,
                      self.batch_size, BINPACK_MIN_PLY, BINPACK_FILTER_CHECKS, BINPACK_SKIP_RATE)
            )
            p.daemon = True
            p.start()
            self.workers.append(p)

        print(f"Started {len(self.workers)} binpack worker processes")

    def stop_workers(self):
        """Stop all worker processes and clean up"""
        print("Stopping workers...")
        self.stop_event.set()

        # Drain the queue to unblock workers
        while not self.data_queue.empty():
            try:
                self.data_queue.get_nowait()
            except:
                break

        # Wait for workers to finish
        for p in self.workers:
            p.join(timeout=5.0)
            if p.is_alive():
                p.terminate()

        self.workers.clear()

        # Clean up queue
        self.data_queue.close()
        self.data_queue.join_thread()

        print("All workers stopped")

    def get_batch(self, timeout: float = 1.0) -> Optional[Dict[str, Any]]:
        """Get a batch from the queue with timeout"""
        wait_start = time.time()

        while True:
            try:
                batch = self.data_queue.get(timeout=timeout)
                wait_time = time.time() - wait_start
                self.stats.main_wait_ms.value += int(wait_time * 1000)
                return batch
            except:
                self.stats.queue_empty_count.value += 1

                # Check if all workers are dead
                alive_workers = sum(1 for p in self.workers if p.is_alive())
                if alive_workers == 0:
                    return None

                if time.time() - wait_start > timeout:
                    return None

    def train_epoch(self, positions_per_epoch: int = POSITIONS_PER_EPOCH) -> Tuple[float, float]:
        """
        Train for one epoch.
        Returns (train_loss, val_loss)
        """
        self.model.train()

        total_train_loss = 0
        train_batch_count = 0
        positions_processed = 0
        gc_counter = 0

        # Clear old validation data
        with self.val_buffer_lock:
            self.val_buffer.clear()

        while positions_processed < positions_per_epoch:
            batch_data = self.get_batch(timeout=QUEUE_READ_TIMEOUT)

            if batch_data is None:
                print("Warning: No batch received, waiting...")
                continue

            process_start = time.time()

            # Train/validation split (main process decides)
            is_validation = self.stats.main_val_batches.value < positions_per_epoch / BATCH_SIZE * self.validation_split

            self.stats.main_batches.value += 1

            if is_validation:
                # Store for validation (with size limit via deque maxlen)
                with self.val_buffer_lock:
                    self.val_buffer.append(batch_data)
                self.stats.main_val_batches.value += 1
            else:
                # Training step
                if NN_TYPE == "NNUE":
                    if self.use_sparse:
                        white_sparse, black_sparse, stm, target = decode_sparse_batch_sparse(
                            batch_data, self.device
                        )
                        self.optimizer.zero_grad()
                        output = self._forward_sparse_nnue(white_sparse, black_sparse, stm)
                        loss = self.criterion(output, target)
                        loss.backward()
                    else:
                        white_input, black_input, stm, target = decode_sparse_batch(
                            batch_data, self.device
                        )
                        self.optimizer.zero_grad()
                        output = self.model(white_input, black_input, stm)
                        loss = self.criterion(output, target)
                        loss.backward()

                    # Gradient clipping for stability
                    if hasattr(self, 'grad_clip') and self.grad_clip > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)

                    self.optimizer.step()

                    total_train_loss += loss.item()
                    train_batch_count += 1
                    positions_processed += batch_data['batch_size']

                    self.stats.main_train_batches.value += 1

                else:  # DNN
                    if self.use_sparse:
                        features_sparse, target = decode_sparse_batch_sparse(
                            batch_data, self.device
                        )
                        self.optimizer.zero_grad()
                        output = self._forward_sparse_dnn(features_sparse)
                        loss = self.criterion(output, target)
                        loss.backward()
                    else:
                        features, target = decode_sparse_batch(
                            batch_data, self.device
                        )
                        self.optimizer.zero_grad()
                        output = self.model(features)
                        loss = self.criterion(output, target)
                        loss.backward()

                    # Gradient clipping for stability
                    if hasattr(self, 'grad_clip') and self.grad_clip > 0:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)

                    self.optimizer.step()

                    total_train_loss += loss.item()
                    train_batch_count += 1
                    positions_processed += batch_data['batch_size']

                    self.stats.main_train_batches.value += 1

            # Clean up batch data
            del batch_data

            process_time = time.time() - process_start
            self.stats.main_process_ms.value += int(process_time * 1000)

            # Progress update
            if train_batch_count != 0 and train_batch_count % int(
                    POSITIONS_PER_EPOCH / BATCH_SIZE / 50) == 0:  # 50 prints per epoch
                avg_loss = total_train_loss / max(1, train_batch_count)
                print(f"  Batch {train_batch_count}: Loss={avg_loss:.6f}, "
                      f"Positions={positions_processed:,}/{positions_per_epoch:,}")

            # Periodic garbage collection
            gc_counter += 1
            if gc_counter >= GC_INTERVAL:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc_counter = 0

        # Calculate validation loss
        val_loss = self._compute_validation_loss()

        avg_train_loss = total_train_loss / max(1, train_batch_count)

        # Clear validation buffer after computing loss
        with self.val_buffer_lock:
            self.val_buffer.clear()
        self.stats.main_val_batches.value = 0

        gc.collect()

        return avg_train_loss, val_loss

    def _forward_sparse_nnue(self, white_sparse: torch.Tensor, black_sparse: torch.Tensor,
                             stm: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for NNUE using sparse input tensors.
        Computes first layer with sparse-dense multiplication for efficiency.
        """
        # First layer: sparse input @ weight.T + bias
        # Use the model's feature transformer (ft) layer weights
        ft_weight = self.model.ft.weight
        ft_bias = self.model.ft.bias

        # Compute first layer for both perspectives using sparse mm
        white_hidden = sparse_first_layer(white_sparse, ft_weight, ft_bias)
        black_hidden = sparse_first_layer(black_sparse, ft_weight, ft_bias)

        # Apply clipped ReLU
        white_hidden = torch.clamp(white_hidden, 0, 1)
        black_hidden = torch.clamp(black_hidden, 0, 1)

        # Concatenate based on side to move (stm=1 for white, stm=0 for black)
        # When white to move: [white, black], when black to move: [black, white]
        combined_white_first = torch.cat([white_hidden, black_hidden], dim=1)
        combined_black_first = torch.cat([black_hidden, white_hidden], dim=1)
        stm_expanded = stm.expand(-1, combined_white_first.size(1))
        combined = torch.where(stm_expanded > 0.5, combined_white_first, combined_black_first)
        # Rest of the network (dense layers)
        x = torch.clamp(self.model.l1(combined), 0, 1)
        x = torch.clamp(self.model.l2(x), 0, 1)
        output = self.model.l3(x)

        return output

    def _forward_sparse_dnn(self, features_sparse: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for DNN using sparse input tensor.
        Computes first layer with sparse-dense multiplication for efficiency.
        """
        # First layer: sparse input @ weight.T + bias
        l1_weight = self.model.l1.weight
        l1_bias = self.model.l1.bias

        x = sparse_first_layer(features_sparse, l1_weight, l1_bias)
        x = torch.clamp(x, 0, 1)

        # Rest of the network (dense layers)
        x = torch.clamp(self.model.l2(x), 0, 1)
        x = torch.clamp(self.model.l3(x), 0, 1)
        output = self.model.l4(x)

        return output

    def _compute_validation_loss(self) -> float:
        """Compute validation loss from buffered batches"""
        self.model.eval()
        total_loss = 0
        batch_count = 0

        with self.val_buffer_lock:
            val_batches = list(self.val_buffer)
            print(f"Computing validation loss, val_batches size={len(val_batches)}...")

        with torch.no_grad():
            for batch_data in val_batches:
                if NN_TYPE == "NNUE":
                    if self.use_sparse:
                        white_sparse, black_sparse, stm, target = decode_sparse_batch_sparse(
                            batch_data, self.device
                        )
                        output = self._forward_sparse_nnue(white_sparse, black_sparse, stm)
                    else:
                        white_input, black_input, stm, target = decode_sparse_batch(
                            batch_data, self.device
                        )
                        output = self.model(white_input, black_input, stm)
                    loss = self.criterion(output, target)
                    total_loss += loss.item()
                    batch_count += 1
                else:  # DNN
                    if self.use_sparse:
                        features_sparse, target = decode_sparse_batch_sparse(
                            batch_data, self.device
                        )
                        output = self._forward_sparse_dnn(features_sparse)
                    else:
                        features, target = decode_sparse_batch(
                            batch_data, self.device
                        )
                        output = self.model(features)
                    loss = self.criterion(output, target)
                    total_loss += loss.item()
                    batch_count += 1

        # Clear the local copy
        del val_batches

        self.model.train()
        return total_loss / max(1, batch_count)

    def train(
            self,
            epochs: int = EPOCHS,
            lr: float = LEARNING_RATE,
            positions_per_epoch: int = POSITIONS_PER_EPOCH,
            early_stopping_patience: int = EARLY_STOPPING_PATIENCE,
            checkpoint_path: str = MODEL_PATH,
            lr_scheduler: str = "plateau",  # "plateau", "step", or "none"
            grad_clip: float = 1.0  # Gradient clipping max norm
    ) -> Dict[str, List[float]]:
        """Main training loop with LR scheduling and gradient clipping"""

        # Update learning rate (unless resuming with saved LR)
        if self.start_epoch == 0:
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

        # Setup learning rate scheduler
        scheduler = None
        if lr_scheduler == "plateau":
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='min', factor=0.5, patience=LR_PATIENCE,
                min_lr=1e-6
            )
        elif lr_scheduler == "step":
            scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=100, gamma=0.5
            )

        self.grad_clip = grad_clip

        early_stopping = EarlyStopping(
            patience=early_stopping_patience,
            checkpoint_path=checkpoint_path,
            optimizer=self.optimizer
        )

        # Initialize early stopping with resumed best loss
        if self.best_val_loss is not None:
            early_stopping.best_loss = self.best_val_loss
            early_stopping.best_epoch = self.start_epoch
            print(f"Resuming with best validation loss: {self.best_val_loss:.6f}")

        history = {'train_loss': [], 'val_loss': [], 'lr': []}

        # Print training mode
        if self.use_sparse:
            print(f"Using SPARSE input optimization (faster)")
        else:
            print(f"Using DENSE input (standard)")

        self.start_workers()

        try:
            for epoch in range(self.start_epoch, epochs):
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"\n{'=' * 60}")
                print(f"Epoch {epoch + 1}/{epochs} (LR: {current_lr:.6f})")
                print('=' * 60)

                train_loss, val_loss = self.train_epoch(positions_per_epoch)

                history['train_loss'].append(train_loss)
                history['val_loss'].append(val_loss)
                history['lr'].append(current_lr)

                print(f"\n  Train Loss: {train_loss:.6f}")
                print(f"  Validation Loss: {val_loss:.6f}")

                # Update learning rate scheduler
                if scheduler is not None:
                    old_lr = self.optimizer.param_groups[0]['lr']
                    if lr_scheduler == "plateau":
                        scheduler.step(val_loss)
                    else:
                        scheduler.step()
                    new_lr = self.optimizer.param_groups[0]['lr']
                    if new_lr != old_lr:
                        print(f"  Learning rate reduced: {old_lr:.6f} -> {new_lr:.6f}")

                if early_stopping(val_loss, self.model, epoch + 1, current_lr):
                    print(f"\nEarly stopping at epoch {epoch + 1}")
                    break

                # Print stats periodically
                if (epoch + 1) % 5 == 0:
                    self.stats.print_stats(self.data_files)

        finally:
            self.stop_workers()
            early_stopping.restore_best_model(self.model)

            # Final stats
            self.stats.print_stats(self.data_files)

        return history


# ==============================================================================
# MAIN ENTRY POINT
# ==============================================================================

if __name__ == "__main__":
    import argparse

    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Train NNUE/DNN chess evaluation network')
    parser.add_argument('--data-dir', type=str, default='./pgn',
                        help='Directory containing binpack training data')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint file to resume training from')
    parser.add_argument('--no-sparse', action='store_true',
                        help='Disable sparse input optimization (use dense tensors)')
    parser.add_argument('--checkpoint', type=str, default=MODEL_PATH,
                        help='Path to save checkpoints')
    parser.add_argument('--epochs', type=int, default=EPOCHS,
                        help='Maximum number of epochs')
    parser.add_argument('--lr', type=float, default=LEARNING_RATE,
                        help='Initial learning rate')
    args = parser.parse_args()

    # Required for multiprocessing on some platforms
    mp.set_start_method('spawn', force=True)

    data_dir = args.data_dir
    use_sparse = not args.no_sparse

    print("=" * 60)
    print(f"{NN_TYPE} Training with Parallel Data Loading")
    print("Data Format: Stockfish Binpack")
    print("=" * 60)

    # Detect device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    print(f"Network type: {NN_TYPE}")
    print(f"Sparse input: {use_sparse}")
    if args.resume:
        print(f"Resuming from: {args.resume}")

    # Create model based on nn_type
    if NN_TYPE == "NNUE":
        model = NNUENetwork(NNUE_INPUT_SIZE, NNUE_HIDDEN_SIZE)
    else:  # DNN
        model = DNNNetwork(DNN_INPUT_SIZE)

    print(f"Model created with {sum(p.numel() for p in model.parameters()):,} parameters")

    # Create trainer
    trainer = ParallelTrainer(
        data_dir=data_dir,
        model=model,
        batch_size=BATCH_SIZE,
        validation_split=VALIDATION_SPLIT,
        queue_size=QUEUE_MAX_SIZE,
        device=device,
        seed=42,
        resume_from=args.resume,
        use_sparse=use_sparse
    )

    # Train with improved parameters
    history = trainer.train(
        epochs=args.epochs,
        lr=args.lr,
        positions_per_epoch=POSITIONS_PER_EPOCH,
        early_stopping_patience=EARLY_STOPPING_PATIENCE,
        checkpoint_path=args.checkpoint,
        lr_scheduler="plateau",
        grad_clip=1.0
    )

    # Summary
    print("\n" + "=" * 60)
    print("TRAINING COMPLETE")
    print("=" * 60)
    print(f"Final train loss: {history['train_loss'][-1]:.6f}")
    print(f"Best validation loss: {min(history['val_loss']):.6f}")
    print(f"Epochs trained: {len(history['train_loss'])}")